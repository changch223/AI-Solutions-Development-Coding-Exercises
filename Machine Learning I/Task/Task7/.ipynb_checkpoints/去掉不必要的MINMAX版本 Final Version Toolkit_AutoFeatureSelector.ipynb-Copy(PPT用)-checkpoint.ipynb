{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a73d6a0-0684-4b0c-b1b3-efd31149eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Feature Selection Methods #####\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import argparse\n",
    "\n",
    "# Pearson Correlation-based feature selection\n",
    "def cor_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Select features based on Pearson correlation coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    - num_feats: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - cor_support: list, boolean mask of selected features.\n",
    "    - cor_feature: list, names of selected features.\n",
    "    \"\"\"\n",
    "    cor_list = [np.corrcoef(X[col], y)[0, 1] for col in X.columns]  # Calculate correlations\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Correlation': np.abs(cor_list)}).sort_values(\n",
    "        'Correlation', ascending=False\n",
    "    )\n",
    "    top_features = feature_value.iloc[:num_feats, :]  # Select top features\n",
    "    cor_support = [col in top_features['Feature'].tolist() for col in X.columns]\n",
    "    cor_feature = X.columns[cor_support].tolist()\n",
    "\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Score': np.abs(cor_list)})\n",
    "    feature_value = feature_value.sort_values('Score', ascending=False)\n",
    "    print(f\"Pearson Correlation-based feature selection\\n{feature_value}\")\n",
    "    return cor_support, cor_feature\n",
    "\n",
    "# Chi-squared test-based feature selection\n",
    "def chi_squared_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Select features based on the chi-squared statistical test.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    - num_feats: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - chi_support: list, boolean mask of selected features.\n",
    "    - chi_feature: list, names of selected features.\n",
    "    \"\"\"\n",
    "    X_norm = MinMaxScaler().fit_transform(X)  # Normalize features\n",
    "    chi_selector = SelectKBest(chi2, k=num_feats).fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:, chi_support].columns.tolist()\n",
    "    scores = chi_selector.scores_\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Score': scores}).sort_values('Score', ascending=False)\n",
    "    print(f\"Chi-squared test-based feature selection\\n{feature_value}\")\n",
    "    return chi_support, chi_feature\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "def rfe_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Select features using RFE with Logistic Regression as the base model.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    - num_feats: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - rfe_support: list, boolean mask of selected features.\n",
    "    - rfe_feature: list, names of selected features.\n",
    "    \"\"\"\n",
    "    X_norm = MinMaxScaler().fit_transform(X)  # Normalize features\n",
    "    rfe_selector = RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=num_feats, step=1).fit(X_norm, y)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:, rfe_support].columns.tolist()\n",
    "    scores = rfe_selector.ranking_\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Score': -scores}).sort_values('Score', ascending=False)\n",
    "    print(f\"Recursive Feature Elimination\\n{feature_value}\")\n",
    "    return rfe_support, rfe_feature\n",
    "\n",
    "# Embedded Logistic Regression (L1 regularization)\n",
    "def embedded_log_reg_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Select features using Logistic Regression with L1 regularization.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    - num_feats: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - embedded_lr_support: list, boolean mask of selected features.\n",
    "    - embedded_lr_feature: list, names of selected features.\n",
    "    \"\"\"\n",
    "    X_norm = MinMaxScaler().fit_transform(X)  # Normalize features\n",
    "    embedded_lr_selector = SelectFromModel(\n",
    "        LogisticRegression(penalty='l1', solver='liblinear', random_state=42),\n",
    "        max_features=num_feats\n",
    "    ).fit(X_norm, y)\n",
    "    embedded_lr_support = embedded_lr_selector.get_support()\n",
    "    embedded_lr_feature = X.loc[:, embedded_lr_support].columns.tolist()\n",
    "\n",
    "\n",
    "    model = LogisticRegression(penalty='l1', solver='liblinear', random_state=42).fit(X_norm, y)\n",
    "    scores = np.abs(model.coef_)[0]\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Score': scores}).sort_values('Score', ascending=False)\n",
    "    print(f\"Embedded Logistic Regression\\n{feature_value}\")\n",
    "    return embedded_lr_support, embedded_lr_feature\n",
    "\n",
    "# Embedded Random Forest feature importance\n",
    "def embedded_rf_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Select features using Random Forest feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    - num_feats: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - embedded_rf_support: list, boolean mask of selected features.\n",
    "    - embedded_rf_feature: list, names of selected features.\n",
    "    \"\"\"\n",
    "    embedded_rf_selector = SelectFromModel(\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        max_features=num_feats\n",
    "    ).fit(X, y)\n",
    "    embedded_rf_support = embedded_rf_selector.get_support()\n",
    "    embedded_rf_feature = X.loc[:, embedded_rf_support].columns.tolist()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42).fit(X, y)\n",
    "    scores = model.feature_importances_\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Score': scores}).sort_values('Score', ascending=False)\n",
    "    print(f\"Embedded Random Forest feature\\n{feature_value}\")    \n",
    "    return embedded_rf_support, embedded_rf_feature\n",
    "\n",
    "# Embedded LightGBM feature importance\n",
    "def embedded_lgbm_selector(X, y, num_feats):\n",
    "    \"\"\"\n",
    "    Select features using LightGBM feature importance.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    - num_feats: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - embedded_lgbm_support: list, boolean mask of selected features.\n",
    "    - embedded_lgbm_feature: list, names of selected features.\n",
    "    \"\"\"\n",
    "    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, random_state=42, verbosity=-1)\n",
    "    embedded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats).fit(X, y)\n",
    "    embedded_lgbm_support = embedded_lgbm_selector.get_support()\n",
    "    embedded_lgbm_feature = X.loc[:, embedded_lgbm_support].columns.tolist()\n",
    "\n",
    "    model = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, random_state=42, verbosity=-1).fit(X, y)\n",
    "    scores = model.feature_importances_\n",
    "    feature_value = pd.DataFrame({'Feature': X.columns, 'Score': scores}).sort_values('Score', ascending=False)\n",
    "    print(f\"Embedded LightGBM feature importance\\n{feature_value}\")   \n",
    "    return embedded_lgbm_support, embedded_lgbm_feature\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset: handle missing values and encode categorical variables.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: DataFrame, the input dataset.\n",
    "\n",
    "    Returns:\n",
    "    - X: DataFrame, feature matrix.\n",
    "    - y: Series, target variable.\n",
    "    \"\"\"\n",
    "    dataset = dataset.dropna(axis=1)  # Drop columns with missing values\n",
    "    y = dataset.iloc[:, -1]  # Assume the last column is the target variable\n",
    "    X = dataset.iloc[:, :-1]  # Feature matrix\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)  # One-hot encoding\n",
    "    return X, y\n",
    "\n",
    "# Auto feature selection\n",
    "def autoFeatureSelector(dataset_path, methods=[], num_output_features=10):\n",
    "    \"\"\"\n",
    "    Perform automatic feature selection using multiple methods.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: DataFrame, input dataset.\n",
    "    - methods: list, feature selection methods to apply (e.g., ['pearson', 'chi-square', ...]).\n",
    "    - num_output_features: int, number of top features to select.\n",
    "\n",
    "    Returns:\n",
    "    - best_features: list, selected feature names.\n",
    "    \"\"\"\n",
    "    X, y = preprocess_dataset(dataset_path)  # Preprocess dataset\n",
    "    feature_name = list(X.columns)\n",
    "    support_dict = {}\n",
    "    feature_dict = {}\n",
    "\n",
    "    # Apply specified feature selection methods\n",
    "    for method in methods:\n",
    "        if method == 'pearson':\n",
    "            support, features = cor_selector(X, y, num_output_features)\n",
    "        elif method == 'chi-square':\n",
    "            support, features = chi_squared_selector(X, y, num_output_features)\n",
    "        elif method == 'rfe':\n",
    "            support, features = rfe_selector(X, y, num_output_features)\n",
    "        elif method == 'log-reg':\n",
    "            support, features = embedded_log_reg_selector(X, y, num_output_features)\n",
    "        elif method == 'rf':\n",
    "            support, features = embedded_rf_selector(X, y, num_output_features)\n",
    "        elif method == 'lgbm':\n",
    "            support, features = embedded_lgbm_selector(X, y, num_output_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        support_dict[method] = support\n",
    "        feature_dict[method] = features\n",
    "\n",
    "    # Create a dataframe summarizing feature selection results\n",
    "    feature_selection_df = pd.DataFrame({'Feature': feature_name})\n",
    "    for method, support in support_dict.items():\n",
    "        feature_selection_df[method] = support\n",
    "    feature_selection_df['Total'] = feature_selection_df.iloc[:, 1:].sum(axis=1)  # Count votes\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Total', 'Feature'], ascending=False)\n",
    "\n",
    "    # Select features with maximum votes\n",
    "    best_features = feature_selection_df.head(num_output_features)['Feature'].tolist()\n",
    "    print(feature_selection_df)  # Print summary table\n",
    "    return best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608899ca-f15e-4646-970c-c34c5639846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation-based feature selection\n",
      "     Feature     Score\n",
      "9    oldpeak  0.438441\n",
      "8      exang  0.438029\n",
      "2         cp  0.434854\n",
      "7    thalach  0.422895\n",
      "11        ca  0.382085\n",
      "10     slope  0.345512\n",
      "12      thal  0.337838\n",
      "1        sex  0.279501\n",
      "0        age  0.229324\n",
      "3   trestbps  0.138772\n",
      "6    restecg  0.134468\n",
      "4       chol  0.099966\n",
      "5        fbs  0.041164\n",
      "Chi-squared test-based feature selection\n",
      "     Feature       Score\n",
      "8      exang  130.470927\n",
      "2         cp   72.607974\n",
      "11        ca   52.656480\n",
      "9    oldpeak   40.911849\n",
      "1        sex   24.373650\n",
      "10     slope   16.836974\n",
      "7    thalach    9.471895\n",
      "12      thal    6.457822\n",
      "6    restecg    4.869671\n",
      "0        age    3.630553\n",
      "3   trestbps    1.517674\n",
      "5        fbs    1.477550\n",
      "4       chol    0.518226\n",
      "Recursive Feature Elimination\n",
      "     Feature  Score\n",
      "2         cp     -1\n",
      "7    thalach     -1\n",
      "9    oldpeak     -1\n",
      "11        ca     -1\n",
      "12      thal     -1\n",
      "3   trestbps     -2\n",
      "4       chol     -3\n",
      "1        sex     -4\n",
      "10     slope     -5\n",
      "8      exang     -6\n",
      "6    restecg     -7\n",
      "0        age     -8\n",
      "5        fbs     -9\n",
      "Embedded Logistic Regression\n",
      "     Feature     Score\n",
      "9    oldpeak  3.060564\n",
      "7    thalach  2.915466\n",
      "11        ca  2.782458\n",
      "2         cp  2.375003\n",
      "12      thal  2.290960\n",
      "1        sex  1.612667\n",
      "4       chol  1.540718\n",
      "3   trestbps  1.527518\n",
      "10     slope  1.095448\n",
      "8      exang  0.942656\n",
      "6    restecg  0.785637\n",
      "0        age  0.163978\n",
      "5        fbs  0.017307\n",
      "Embedded Random Forest feature\n",
      "     Feature     Score\n",
      "2         cp  0.134201\n",
      "7    thalach  0.120473\n",
      "11        ca  0.116755\n",
      "9    oldpeak  0.116151\n",
      "12      thal  0.097043\n",
      "0        age  0.089313\n",
      "4       chol  0.078930\n",
      "3   trestbps  0.074253\n",
      "8      exang  0.059592\n",
      "10     slope  0.048738\n",
      "1        sex  0.036057\n",
      "6    restecg  0.019619\n",
      "5        fbs  0.008874\n",
      "Embedded LightGBM feature importance\n",
      "     Feature  Score\n",
      "7    thalach   1883\n",
      "4       chol   1857\n",
      "0        age   1817\n",
      "3   trestbps   1457\n",
      "9    oldpeak   1410\n",
      "2         cp    760\n",
      "11        ca    712\n",
      "12      thal    622\n",
      "1        sex    449\n",
      "6    restecg    399\n",
      "8      exang    322\n",
      "10     slope    320\n",
      "5        fbs    133\n",
      "     Feature  pearson  chi-square    rfe  log-reg     rf   lgbm  Total\n",
      "9    oldpeak     True        True   True     True   True   True      6\n",
      "7    thalach     True       False   True     True   True   True      5\n",
      "2         cp     True        True   True     True   True  False      5\n",
      "11        ca     True        True   True     True   True  False      5\n",
      "12      thal    False       False   True     True   True  False      3\n",
      "8      exang     True        True  False    False  False  False      2\n",
      "3   trestbps    False       False  False    False  False   True      1\n",
      "1        sex    False        True  False    False  False  False      1\n",
      "4       chol    False       False  False    False  False   True      1\n",
      "0        age    False       False  False    False  False   True      1\n",
      "10     slope    False       False  False    False  False  False      0\n",
      "6    restecg    False       False  False    False  False  False      0\n",
      "5        fbs    False       False  False    False  False  False      0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['oldpeak', 'thalach', 'cp', 'ca', 'thal']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Find Best Features #####\n",
    "\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "# `num_output_features` specifies the number of features to select for each method.\n",
    "# The `autoFeatureSelector` function applies multiple feature selection methods \n",
    "# (e.g., Pearson correlation, chi-square test, RFE, etc.) and combines their results\n",
    "# by voting to identify the most important features.\n",
    "\n",
    "best_features = autoFeatureSelector(\n",
    "    df,\n",
    "    methods=['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm'],  # List of feature selection methods\n",
    "    num_output_features=5  # Number of features to select per method\n",
    ")\n",
    "\n",
    "# Output the final list of selected features after voting\n",
    "best_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f074b8-4a78-45de-87ad-cb2232e4b441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
