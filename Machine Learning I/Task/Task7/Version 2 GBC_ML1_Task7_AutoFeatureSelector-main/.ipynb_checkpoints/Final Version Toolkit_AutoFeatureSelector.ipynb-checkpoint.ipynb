{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6a73d6a0-0684-4b0c-b1b3-efd31149eafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE, SelectFromModel \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import argparse\n",
    "\n",
    "def cor_selector(X, y, num_feats):\n",
    "    # Normalize X using MinMaxScaler\n",
    "    X_norm = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    cor_list = []\n",
    "    feature_name = X_norm.columns.tolist()\n",
    "    # calculate correlation between features and target\n",
    "    for i in feature_name:\n",
    "        cor = np.corrcoef(X[i], y)[0, 1]\n",
    "        cor_list.append(cor)\n",
    "        \n",
    "    # convert to dataframe\n",
    "    feature_value = pd.DataFrame(\n",
    "        {'Feature': feature_name,\n",
    "         'Correlation': cor_list})\n",
    "    \n",
    "    # sort by absolute correlation value\n",
    "    feature_value['Correlation'] = feature_value['Correlation'].abs()\n",
    "    feature_value = feature_value.sort_values('Correlation', ascending=False)\n",
    "    \n",
    "    # select top features\n",
    "    topk_feature = feature_value.iloc[:num_feats, :]\n",
    "    \n",
    "    # create boolean mask\n",
    "    cor_support = []\n",
    "    for feat in feature_name:\n",
    "        if feat in topk_feature['Feature'].tolist():\n",
    "            cor_support.append(True)\n",
    "        else:\n",
    "            cor_support.append(False)\n",
    "            \n",
    "    # get selected feature names\n",
    "    cor_feature = X.columns[cor_support].tolist()\n",
    "    \n",
    "    return cor_support, cor_feature\n",
    "\n",
    "def chi_squared_selector(X, y, num_feats):\n",
    "    # Normalize X using MinMaxScaler\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
    "    chi_selector.fit(X_norm, y)\n",
    "    chi_support = chi_selector.get_support()\n",
    "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
    "    return chi_support, chi_feature\n",
    "\n",
    "def rfe_selector(X, y, num_feats):\n",
    "    # Normalize X using MinMaxScaler\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    rfe_selector = RFE(\n",
    "        estimator=LogisticRegression(random_state=42),\n",
    "        n_features_to_select=num_feats,\n",
    "        step=1\n",
    "    )\n",
    "    rfe_selector.fit(X_norm, y)\n",
    "    rfe_support = rfe_selector.get_support()\n",
    "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
    "    return rfe_support, rfe_feature\n",
    "\n",
    "def embedded_log_reg_selector(X, y, num_feats):\n",
    "    # Normalize X using MinMaxScaler\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    embedded_lr_selector = SelectFromModel(\n",
    "        LogisticRegression(penalty='l1', solver='liblinear', random_state=42),\n",
    "        max_features=num_feats\n",
    "    )\n",
    "    embedded_lr_selector.fit(X_norm, y)\n",
    "    embedded_lr_support = embedded_lr_selector.get_support()\n",
    "    embedded_lr_feature = X.loc[:,embedded_lr_support].columns.tolist()\n",
    "    return embedded_lr_support, embedded_lr_feature\n",
    "\n",
    "def embedded_rf_selector(X, y, num_feats):\n",
    "    # Normalize X using MinMaxScaler\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    embedded_rf_selector = SelectFromModel(\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        max_features=num_feats\n",
    "    )\n",
    "    embedded_rf_selector.fit(X_norm, y)\n",
    "    embedded_rf_support = embedded_rf_selector.get_support()\n",
    "    embedded_rf_feature = X.loc[:,embedded_rf_support].columns.tolist()\n",
    "    return embedded_rf_support, embedded_rf_feature\n",
    "\n",
    "def embedded_lgbm_selector(X, y, num_feats):\n",
    "    # Normalize X using MinMaxScaler\n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, random_state=42, verbosity=-1) # Verbosity = -1 to suppress warnings\n",
    "    embedded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
    "    embedded_lgbm_selector.fit(X_norm, y)\n",
    "    embedded_lgbm_support = embedded_lgbm_selector.get_support()\n",
    "    embedded_lgbm_feature = X.loc[:,embedded_lgbm_support].columns.tolist()\n",
    "    return embedded_lgbm_support, embedded_lgbm_feature\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    # Drop any column that contains missing values\n",
    "    dataset = dataset.dropna(axis=1)\n",
    "    \n",
    "    # Assuming the last column is the target variable\n",
    "    y = dataset.iloc[:, -1]\n",
    "    \n",
    "    # Identify categorical columns and perform one-hot encoding\n",
    "    X = dataset.iloc[:, :-1]\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "    X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)   \n",
    "\n",
    "    return X, y\n",
    "\n",
    "def autoFeatureSelector(dataset_path, methods=[],num_output_features=10):\n",
    "    # Parameters\n",
    "    # data - dataset to be analyzed (csv file)\n",
    "    # methods - various feature selection methods we outlined before, use them all here (list)\n",
    "\n",
    "    num_feats = num_output_features\n",
    "    \n",
    "    # preprocessing\n",
    "    X, y = preprocess_dataset(dataset_path)\n",
    "    feature_name = list(X.columns)\n",
    "    \n",
    "    # Dictionary to store support indicators for each method\n",
    "    support_dict = {}\n",
    "    feature_dict = {}\n",
    "    \n",
    "    # Run every method we outlined above from the methods list and collect returned best features from every method\n",
    "    if 'pearson' in methods:\n",
    "        cor_support, cor_feature = cor_selector(X, y,num_feats)\n",
    "        support_dict['pearson'] = cor_support\n",
    "        feature_dict['pearson'] = cor_feature\n",
    "    if 'chi-square' in methods:\n",
    "        chi_support, chi_feature = chi_squared_selector(X, y,num_feats)\n",
    "        support_dict['chi-square'] = chi_support\n",
    "        feature_dict['chi-square'] = chi_feature\n",
    "    if 'rfe' in methods:\n",
    "        rfe_support, rfe_feature = rfe_selector(X, y,num_feats)\n",
    "        support_dict['rfe'] = rfe_support\n",
    "        feature_dict['rfe'] = rfe_feature\n",
    "    if 'log-reg' in methods:\n",
    "        embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n",
    "        support_dict['log-reg'] = embedded_lr_support\n",
    "        feature_dict['log-reg'] = embedded_lr_feature\n",
    "    if 'rf' in methods:\n",
    "        embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n",
    "        support_dict['rf'] = embedded_rf_support\n",
    "        feature_dict['rf'] = embedded_rf_feature\n",
    "    if 'lgbm' in methods:\n",
    "        embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n",
    "        support_dict['lgbm'] = embedded_lgbm_support\n",
    "        feature_dict['lgbm'] = embedded_lgbm_feature\n",
    "    \n",
    "    # Create dataframe with all selection methods\n",
    "    feature_selection_df = pd.DataFrame({'Feature':feature_name})\n",
    "    for method in methods:\n",
    "        feature_selection_df[method] = support_dict[method]\n",
    "    \n",
    "    # Count the total votes for each feature\n",
    "    feature_selection_df['Total'] = feature_selection_df.iloc[:, 1:].sum(axis=1)\n",
    "    \n",
    "    # Sort features by total votes and feature name\n",
    "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'], ascending=False)\n",
    "    \n",
    "    # Select features with maximum votes\n",
    "    max_votes = feature_selection_df['Total'].max()\n",
    "    best_features = feature_selection_df.head(num_feats)['Feature'].tolist()\n",
    "\n",
    "    print(feature_selection_df)\n",
    "    \n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "608899ca-f15e-4646-970c-c34c5639846c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature  pearson  chi-square    rfe  log-reg     rf   lgbm  Total\n",
      "9    oldpeak     True        True   True     True   True   True      6\n",
      "7    thalach     True       False   True     True   True   True      5\n",
      "2         cp     True        True   True     True   True  False      5\n",
      "11        ca     True        True   True     True   True  False      5\n",
      "12      thal    False       False   True     True   True  False      3\n",
      "8      exang     True        True  False    False  False  False      2\n",
      "3   trestbps    False       False  False    False  False   True      1\n",
      "1        sex    False        True  False    False  False  False      1\n",
      "4       chol    False       False  False    False  False   True      1\n",
      "0        age    False       False  False    False  False   True      1\n",
      "10     slope    False       False  False    False  False  False      0\n",
      "6    restecg    False       False  False    False  False  False      0\n",
      "5        fbs    False       False  False    False  False  False      0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['oldpeak', 'thalach', 'cp', 'ca', 'thal']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"heart.csv\")\n",
    "best_features = autoFeatureSelector(df, methods=['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm'],num_output_features=5)\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1a39a-9f56-4d8d-90f1-3d099e84d09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
